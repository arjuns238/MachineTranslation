{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPameBUcU9NwTIxRtpwkgy0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjuns238/MachineTranslation/blob/main/MachineTranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mdoKzChr3DqN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from torch.utils.data import Dataset, DataLoader,random_split\n",
        "from typing import Iterable, List\n",
        "import torchtext\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Installing dependencies\n",
        "# !pip install -U torchdata\n",
        "# !pip install -U spacy\n",
        "!pip install 'portalocker>=2.0.0'\n",
        "# !python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euelWWGW4DW9",
        "outputId": "b1d87a4e-721e-4db2-ae18-c6246a5728aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting fr-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.7.0/fr_core_news_sm-3.7.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->fr-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "block_size = 128\n",
        "learning_rate = 1e-2\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 384\n",
        "dropout = 0.2\n",
        "no_of_heads = 6\n",
        "n_layer = 6\n",
        "device\n",
        "SRC_LANGUAGE = 'Fr'\n",
        "TGT_LANGUAGE = 'En'\n",
        "# device = xm.xla_device()\n",
        "device"
      ],
      "metadata": {
        "id": "1oMLIZgl3Joe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a75f93d6-7ade-4fa7-9e14-ee8a9c88a539"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/eng-fra.txt\", sep=\"\\t\", header=None)\n",
        "data = data.set_axis(['En','Fr'], axis = 1) # Rename indices\n",
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UBZLUOIp3Ltw",
        "outputId": "caccf22a-0afc-4b81-aba8-a03f6418ccb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                       En  \\\n",
              "135837  A carbon footprint is the amount of carbon dio...   \n",
              "135838  Death is something that we're often discourage...   \n",
              "135839  Since there are usually multiple websites on a...   \n",
              "135840  If someone who doesn't know your background sa...   \n",
              "135841  It may be impossible to get a completely error...   \n",
              "\n",
              "                                                       Fr  \n",
              "135837  Une empreinte carbone est la somme de pollutio...  \n",
              "135838  La mort est une chose qu'on nous décourage sou...  \n",
              "135839  Puisqu'il y a de multiples sites web sur chaqu...  \n",
              "135840  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
              "135841  Il est peut-être impossible d'obtenir un Corpu...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2f55703-7188-491f-accd-a6a01bb4a892\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>En</th>\n",
              "      <th>Fr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>135837</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135838</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135839</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135840</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135841</th>\n",
              "      <td>It may be impossible to get a completely error...</td>\n",
              "      <td>Il est peut-être impossible d'obtenir un Corpu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2f55703-7188-491f-accd-a6a01bb4a892')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a2f55703-7188-491f-accd-a6a01bb4a892 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a2f55703-7188-491f-accd-a6a01bb4a892');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-88523c71-bfe3-46d3-ac18-15c3d9ef68a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-88523c71-bfe3-46d3-ac18-15c3d9ef68a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-88523c71-bfe3-46d3-ac18-15c3d9ef68a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"En\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\",\n          \"It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.\",\n          \"Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fr\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"La mort est une chose qu'on nous d\\u00e9courage souvent de discuter ou m\\u00eame de penser mais j'ai pris conscience que se pr\\u00e9parer \\u00e0 la mort est l'une des choses que nous puissions faire qui nous investit le plus de responsabilit\\u00e9. R\\u00e9fl\\u00e9chir \\u00e0 la mort clarifie notre vie.\",\n          \"Il est peut-\\u00eatre impossible d'obtenir un Corpus compl\\u00e8tement d\\u00e9nu\\u00e9 de fautes, \\u00e9tant donn\\u00e9e la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres \\u00e0 produire des phrases dans leurs propres langues plut\\u00f4t que d'exp\\u00e9rimenter dans les langues qu'ils apprennent, nous pourrions \\u00eatre en mesure de r\\u00e9duire les erreurs.\",\n          \"Puisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arri\\u00e8re lorsque j'atterris sur n'importe quelle page qui contient des publicit\\u00e9s surgissantes. Je me rends juste sur la prochaine page propos\\u00e9e par Google et esp\\u00e8re tomber sur quelque chose de moins irritant.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='fr_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter[language]:\n",
        "        yield token_transform[language](data_sample)\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    print(ln)\n",
        "    # Training data Iterator\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(data, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS6lnvtw3OKh",
        "outputId": "dddd8c09-e983-4ce6-ca9d-8327100d348e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fr\n",
            "En\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_transform[\"En\"].lookup_token(200))\n",
        "print(vocab_transform[\"En\"].lookup_indices([\"left\"]))\n",
        "vocab_size_src = len(vocab_transform[SRC_LANGUAGE])\n",
        "vocab_size_tgt = len(vocab_transform[TGT_LANGUAGE])\n",
        "print(f\"Vocab size for {SRC_LANGUAGE} = {vocab_size_src}\")\n",
        "print(f\"Vocab size for {TGT_LANGUAGE} = {vocab_size_tgt}\")"
      ],
      "metadata": {
        "id": "4GlvmB6N3RP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed195a8b-5951-410f-8a55-ab24bbdd46ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "left\n",
            "[200]\n",
            "Vocab size for Fr = 24554\n",
            "Vocab size for En = 14875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Encode a sentence\n",
        "def encode_sentence(sentence: str, language: str, tokenizer, vocab) -> List[int]:\n",
        "    # Tokenize the sentence\n",
        "    tokens = tokenizer(sentence)\n",
        "    # Convert tokens to indices using vocabulary\n",
        "    indices = vocab_transform[language].lookup_indices(tokens)\n",
        "    return indices\n",
        "\n",
        "# Step 2: Decode a sequence\n",
        "def decode_sequence(indices: List[int], language: str, vocab) -> str:\n",
        "    # Convert indices to tokens\n",
        "    tokens = [vocab_transform[language].lookup_token(index) for index in indices]\n",
        "    # Remove <bos> and <eos> tokens if present\n",
        "    if tokens[0] == '<bos>':\n",
        "        tokens = tokens[1:]\n",
        "    if tokens[-1] == '<eos>':\n",
        "        tokens = tokens[:-1]\n",
        "    # Convert tokens to a sentence\n",
        "    sentence = \"\"\n",
        "    for token in tokens:\n",
        "        if token == '<bos>' or  token == '<eos>' or token == '<pad>':\n",
        "            continue\n",
        "        sentence = sentence + \" \" + token\n",
        "    return sentence\n",
        "\n",
        "# Example usage\n",
        "sentence = \"Je suis froid\"\n",
        "encoded = encode_sentence(sentence, SRC_LANGUAGE, token_transform[SRC_LANGUAGE], vocab_transform[SRC_LANGUAGE])\n",
        "decoded = decode_sequence(encoded, SRC_LANGUAGE, vocab_transform[SRC_LANGUAGE])\n",
        "print(\"Original sentence:\", sentence)\n",
        "print(\"Encoded sequence:\", encoded)\n",
        "print(\"Decoded sentence:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B2aBecu3ZXu",
        "outputId": "d62daf32-b67b-4339-f72a-40074c2893f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: Je suis froid\n",
            "Encoded sequence: [6, 34, 448]\n",
            "Decoded sentence:  Je suis froid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_sample = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
        "        src_sample = src_sample[:block_size]\n",
        "        tgt_sample = text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\"))\n",
        "        tgt_sample = tgt_sample[:block_size]\n",
        "        src_batch.append(src_sample)\n",
        "        tgt_batch.append(tgt_sample)\n",
        "\n",
        "        # src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        # tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "\n",
        "    return src_batch.T, tgt_batch.T"
      ],
      "metadata": {
        "id": "K530InMa3br6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen = data[\"Fr\"][0]\n",
        "sen = data[\"En\"][0]\n",
        "\n",
        "print(sen)\n",
        "outputText = text_transform[TGT_LANGUAGE](sen.rstrip(\"\\n\"))\n",
        "engText = text_transform[SRC_LANGUAGE](sen.rstrip(\"\\n\"))\n",
        "\n",
        "print(outputText)\n",
        "print(engText)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu8cT3jefz-J",
        "outputId": "28f1b25e-6f0f-4e34-f73d-2fcf93313aac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go.\n",
            "tensor([  2, 572,   4,   3])\n",
            "tensor([2, 0, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputText, outputText):\n",
        "        self.inputText = inputText\n",
        "        self.outputText = outputText\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputText)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.inputText[idx]\n",
        "        outputText = self.outputText[idx]\n",
        "        return x, outputText\n",
        "\n",
        "dataset = CustomDataset(data[\"Fr\"], data[\"En\"])\n"
      ],
      "metadata": {
        "id": "VHHPSEEv3epu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset:\n",
        "    print(\"x = \", x)\n",
        "    print(\"y = \", y)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrhwjMSwfchZ",
        "outputId": "6bd08670-c46f-4186-a885-7993dad67bfc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x =  Va !\n",
            "y =  Go.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, validation split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create a DataLoader to iterate over batches of data and performing preprocessing - By default produces batch first\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn = collate_fn, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "XD6-Dd2wY2CP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for x, y in train_dataloader:\n",
        "    print(x.shape)\n",
        "    for src in x:\n",
        "        print(\"X input\", decode_sequence(src, SRC_LANGUAGE, vocab_transform[SRC_LANGUAGE]))\n",
        "        # print(\"Y input\", decode_sequence(tgt, TGT_LANGUAGE, vocab_transform[TGT_LANGUAGE]))\n",
        "    print(\"y shape = \", y.shape)\n",
        "    for item in y:\n",
        "        print(\"Y labels\", decode_sequence(item.T, TGT_LANGUAGE, vocab_transform[TGT_LANGUAGE]))\n",
        "    if i == 1:\n",
        "        break\n",
        "    i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCY4E-m0S1-v",
        "outputId": "afb92fc0-6fa6-4507-c1c6-04e3f6ff3dff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 18])\n",
            "X input  Je veux m' entretenir avec toi à propos de ce rapport .\n",
            "X input  Le chien fut heurté par un camion .\n",
            "X input  Il a l' air malade .\n",
            "X input  Nous finirons par faire de toi un matelot .\n",
            "X input  Le latin est une langue morte .\n",
            "X input  L' Espagne est un pays développé .\n",
            "X input  Je le mérite .\n",
            "X input  Quel est le problème de Tom ?\n",
            "X input  Qui le fera   ?\n",
            "X input  Verriez -vous un inconvénient à ce que je boive le reste du lait ?\n",
            "X input  Je l' ai vu traverser la route .\n",
            "X input  Je serai direct .\n",
            "X input  Le Japon joue un rôle clé dans l' économie mondiale .\n",
            "X input  Nous disposons de plein de temps libre .\n",
            "X input  Je me suis fait étendre .\n",
            "X input  Tom est heureux .\n",
            "X input  Nous sortons prendre quelque chose à manger .\n",
            "X input  Elles ne peuvent te faire de mal .\n",
            "X input  En joue ! Feu !\n",
            "X input  Sortons un moment nous promener .\n",
            "X input  Elle fera de son mieux pour être là à l' heure .\n",
            "X input  Il m' a confectionné un gâteau .\n",
            "X input  Cessez de vous conduire comme une enfant !\n",
            "X input  Voici ton livre .\n",
            "X input  Il vient parfois me rendre visite .\n",
            "X input  Va -t -il mieux aujourd'hui ?\n",
            "X input  Arrivez -vous à lire ça ?\n",
            "X input  Nous partirons demain , si le temps le permet .\n",
            "X input  Je n' étais le seul à être en retard .\n",
            "X input  Ça ne s' est pas bien passé .\n",
            "X input  Elle était à la hauteur du boulot .\n",
            "X input  Ma mère est morte durant mon absence .\n",
            "X input  Vous auriez dû lui en parler tandis qu' il était là .\n",
            "X input  Ils ont fouillé ici et là à la recherche de survivants .\n",
            "X input  Point n' est besoin de nous presser .\n",
            "X input  Je n' ai pas de temps à consacrer aux garçons .\n",
            "X input  Ton téléphone sonne .\n",
            "X input  J' imagine que c' est possible .\n",
            "X input  Je pensais que ce serait simple à faire .\n",
            "X input  Ce sont les miennes .\n",
            "X input  Je vis près de la mer alors je me rends souvent à la plage .\n",
            "X input  Nous t' aimons tant .\n",
            "X input  Elle ne put se retenir de rire .\n",
            "X input  Tout ce que je veux , c' est quelqu' un de spécial dans ma vie .\n",
            "X input  Le printemps est la saison que j' aime le mieux .\n",
            "X input  Tu as peut-être raison , mais nous avons une opinion légèrement différente .\n",
            "X input  Je vais voir s' il est là .\n",
            "X input  Je ne suis pas extravertie .\n",
            "X input  Il n' est pas là actuellement .\n",
            "X input  Elle le fit chanter .\n",
            "X input  Ça ne signifie pas que vous ne devriez pas être prudent .\n",
            "X input  Que penses - tu de lui   ?\n",
            "X input  Nous avons une bonne équipe et tout le monde le sait .\n",
            "X input  Tom se mit à sauter partout .\n",
            "X input  Il est apparu de nulle part .\n",
            "X input  Tom chanta mieux que Marie .\n",
            "X input  Je veux rester avec vous .\n",
            "X input  Vous n' êtes pas préparé à ce qui vous attend .\n",
            "X input  Veuillez regarder le tableau , tous .\n",
            "X input  Le gâteau est délicieux .\n",
            "X input  C' est une fondue de tricot .\n",
            "X input  Je cédais à la tentation et me mis à fumer une cigarette .\n",
            "X input  J' ai rencontré une vieille femme .\n",
            "X input  J' essaye de ne pas pleurer .\n",
            "y shape =  torch.Size([64, 15])\n",
            "Y labels  I want to talk to you about this report .\n",
            "Y labels  The dog was hit by a truck .\n",
            "Y labels  He looks as if he were ill .\n",
            "Y labels  We 'll make a sailor out of you yet .\n",
            "Y labels  Latin is a dead language .\n",
            "Y labels  Spain is a developed country .\n",
            "Y labels  I deserve it .\n",
            "Y labels  What 's Tom 's problem ?\n",
            "Y labels  Who will do it ?\n",
            "Y labels  Would you mind if I drank the rest of the milk ?\n",
            "Y labels  I saw him cross the street .\n",
            "Y labels  I 'll be direct .\n",
            "Y labels  Japan plays a key role in the world economy .\n",
            "Y labels  We have plenty of time to spare .\n",
            "Y labels  I was knocked unconscious .\n",
            "Y labels  Tom 's glad .\n",
            "Y labels  We 're going out to get something to eat .\n",
            "Y labels  They ca n't hurt you .\n",
            "Y labels  Aim . Fire !\n",
            "Y labels  Let 's get out for a while to take a walk .\n",
            "Y labels  She will do her best to be here on time .\n",
            "Y labels  He made me a cake .\n",
            "Y labels  Quit acting like a child .\n",
            "Y labels  Here is your book .\n",
            "Y labels  He sometimes drops in on me .\n",
            "Y labels  Is he any better today ?\n",
            "Y labels  Can you read that ?\n",
            "Y labels  We 'll leave tomorrow , weather permitting .\n",
            "Y labels  I was n't the only one who was late .\n",
            "Y labels  It did n't go well .\n",
            "Y labels  She was equal to the job .\n",
            "Y labels  My mother died during my absence .\n",
            "Y labels  You should have told him about it while he was here .\n",
            "Y labels  They searched here and there looking for survivors .\n",
            "Y labels  There 's no need to hurry .\n",
            "Y labels  I do n't have time for boys .\n",
            "Y labels  Your phone 's ringing .\n",
            "Y labels  I guess it 's possible .\n",
            "Y labels  I thought it would be easy to do .\n",
            "Y labels  They 're mine .\n",
            "Y labels  I live near the sea so I often go to the beach .\n",
            "Y labels  We love you so much .\n",
            "Y labels  She could n't hold back her laughter .\n",
            "Y labels  All I want is someone special in my life .\n",
            "Y labels  I like spring the best of the seasons .\n",
            "Y labels  You may be right , but we have a slightly different opinion .\n",
            "Y labels  I 'll see if he is in .\n",
            "Y labels  I 'm not outgoing .\n",
            "Y labels  He is n't here now .\n",
            "Y labels  She blackmailed him .\n",
            "Y labels  That does n't mean you should n't be careful .\n",
            "Y labels  What do you think of him ?\n",
            "Y labels  We have a good team and everyone knows it .\n",
            "Y labels  Tom started shaking uncontrollably .\n",
            "Y labels  He appeared from nowhere .\n",
            "Y labels  Tom sang better than Mary did .\n",
            "Y labels  I want to stay with you .\n",
            "Y labels  You 're not prepared for what awaits you .\n",
            "Y labels  Look at the blackboard , everyone .\n",
            "Y labels  The cake is delicious .\n",
            "Y labels  She is really into knitting .\n",
            "Y labels  I gave in to temptation and began to smoke a cigarette .\n",
            "Y labels  I met an old woman .\n",
            "Y labels  I 'm trying not to cry .\n",
            "torch.Size([64, 18])\n",
            "X input  Nous avons perdu une bataille , mais nous gagnerons la guerre .\n",
            "X input  Est -ce que quelqu' un peut m' aider   ? \" Je veux bien . \"\n",
            "X input  Elle l' a sermonné pour son retard .\n",
            "X input  Veux - tu me garder mes effets de valeur , s' il te plait ?\n",
            "X input  Je ne puis accepter ton cadeau .\n",
            "X input  Il étudie l' histoire à l' université .\n",
            "X input  Nous nous sommes rencontrés la semaine dernière .\n",
            "X input  J' ai une fille .\n",
            "X input  Je suis vraiment content que vous soyez là .\n",
            "X input  Je pense que Tom est dans son bureau .\n",
            "X input  Les élèves se lèvent lorsque leur professeur entre .\n",
            "X input  Je n' ai pas de tatouage .\n",
            "X input  Qu' y a -t -il derrière la porte   ?\n",
            "X input  Je suis content que tout se soit bien passé .\n",
            "X input  Je veux que tu descendes de mon dos .\n",
            "X input  Il s' approcha d' elle .\n",
            "X input  Mettez l' échelle contre le mur .\n",
            "X input  Quelle déception !\n",
            "X input  J' avais entendu que tu avais changé .\n",
            "X input  Je ne me suis jamais si bien portée .\n",
            "X input  Elle ne vint pas avant deux heures .\n",
            "X input  Il sait courir plus vite que moi .\n",
            "X input  N' importe qui peut poser une question .\n",
            "X input  Elles ont souri .\n",
            "X input  Cela vous a -t -il semblé bizarre ?\n",
            "X input  C' est le village où j' ai passé mon enfance .\n",
            "X input  Ce n' est pas ce que j' ai commandé .\n",
            "X input  Le sucre remplaça le miel comme édulcorant .\n",
            "X input  La réunion était sur le point de se terminer .\n",
            "X input  J' ai mal ici .\n",
            "X input  Le train arriva finalement .\n",
            "X input  Nous avons partagé le prix du repas .\n",
            "X input  J' ai laissé mon dictionnaire en bas .\n",
            "X input  Je pense que Tom vous cherchait .\n",
            "X input  Allez chercher vos affaires !\n",
            "X input  Peut - être pouvez -vous me battre .\n",
            "X input  Tom était insatisfait des résultats .\n",
            "X input  Quelle était ta question ?\n",
            "X input  Il se pourrait même que ça ne fonctionne pas .\n",
            "X input  Je ne suis pas à l' aise avec ça .\n",
            "X input  Je n' ai même pas remarqué que vous étiez là .\n",
            "X input  Il fait chaud ici .\n",
            "X input  Elle n' est plus la femme gaie qu' elle était .\n",
            "X input  Je dois l' avertir .\n",
            "X input  Êtes -vous encore contrariés à propos de ce qui s' est passé ?\n",
            "X input  Ce fut naïf de ma part .\n",
            "X input  Vous ne comprenez pas .\n",
            "X input  Ne jamais sous-estimer Tom .\n",
            "X input  Plus tu regarderas et plus tu verras , et plus intéressant encore ils deviendront .\n",
            "X input  Dormez bien .\n",
            "X input  Pouvons -nous aller à l' intérieur ?\n",
            "X input  Tom boutonna son manteau .\n",
            "X input  L’ Arménie a rejoint l’ Organisation mondiale du commerce en 2003 .\n",
            "X input  Si seulement j' avais su alors ce que je sais désormais .\n",
            "X input  Nous sommes allés ensemble à la plage .\n",
            "X input  J' ai quelque chose d' intéressant à vous conter que vous pourriez trouver surprenant .\n",
            "X input  Es - tu en train de me dire que tu n' es pas impliqué ?\n",
            "X input  Pourquoi ne peux - tu pas le faire ?\n",
            "X input  Pour moi , c' est la fin des haricots .\n",
            "X input  Un coca s' il vous plaît .\n",
            "X input  Cherches - tu quelqu' un ?\n",
            "X input  On l' a vu aux nouvelles .\n",
            "X input  Tom et Mary cherchent John .\n",
            "X input  As - tu un animal de compagnie   ?\n",
            "y shape =  torch.Size([64, 21])\n",
            "Y labels  We have lost a battle , but we will win the war .\n",
            "Y labels  Can somebody help me ? \" I will . \"\n",
            "Y labels  She scolded him for being late .\n",
            "Y labels  Will you keep my valuables for me , please ?\n",
            "Y labels  I ca n't accept your gift .\n",
            "Y labels  He is studying history at the university .\n",
            "Y labels  We met last week .\n",
            "Y labels  I have a daughter .\n",
            "Y labels  I really am glad you 're here .\n",
            "Y labels  I think Tom is in his office .\n",
            "Y labels  Students stand up when their teacher enters .\n",
            "Y labels  I do n't have a tattoo .\n",
            "Y labels  What 's behind the door ?\n",
            "Y labels  I 'm just glad everything worked out .\n",
            "Y labels  I want you to get off my back .\n",
            "Y labels  He moved close to her .\n",
            "Y labels  Place the ladder against the wall .\n",
            "Y labels  What a letdown !\n",
            "Y labels  I 'd heard you 'd changed .\n",
            "Y labels  I 've never been better .\n",
            "Y labels  She did not come until two .\n",
            "Y labels  He can run faster than me .\n",
            "Y labels  Anyone can ask a question .\n",
            "Y labels  They smiled .\n",
            "Y labels  Was that weird for you ?\n",
            "Y labels  This is the village where I spent my childhood .\n",
            "Y labels  This is not what I ordered .\n",
            "Y labels  Sugar replaced honey as a sweetener .\n",
            "Y labels  The meeting was almost over .\n",
            "Y labels  I have a pain here .\n",
            "Y labels  The train finally arrived .\n",
            "Y labels  We shared the cost of the meal .\n",
            "Y labels  I left my dictionary downstairs .\n",
            "Y labels  I think Tom was looking for you .\n",
            "Y labels  Get your gear .\n",
            "Y labels  Perhaps you can beat me .\n",
            "Y labels  Tom was unsatisfied with the results .\n",
            "Y labels  What was your question ?\n",
            "Y labels  It might not even work .\n",
            "Y labels  I 'm not comfortable with this .\n",
            "Y labels  I did n't even notice you were there .\n",
            "Y labels  It 's warm in here .\n",
            "Y labels  She is no longer the cheerful woman she once was .\n",
            "Y labels  I have to warn him .\n",
            "Y labels  Are you still upset about what happened ?\n",
            "Y labels  That was naive of me .\n",
            "Y labels  You do n't understand .\n",
            "Y labels  Do n't ever underestimate Tom .\n",
            "Y labels  The more you look , the more you will see , and the more interesting they will become .\n",
            "Y labels  Sleep tight .\n",
            "Y labels  Can we go inside ?\n",
            "Y labels  Tom buttoned his coat .\n",
            "Y labels  Armenia joined the World Trade Organization in 2003 .\n",
            "Y labels  If only I 'd known then what I know now .\n",
            "Y labels  We went to the beach together .\n",
            "Y labels  I 've something interesting to tell you that you might find surprising .\n",
            "Y labels  Are you telling me you 're not involved ?\n",
            "Y labels  Why ca n't you do it ?\n",
            "Y labels  I 'm at the end of my rope .\n",
            "Y labels  A coke , please .\n",
            "Y labels  Are you looking for someone ?\n",
            "Y labels  We saw it on the news .\n",
            "Y labels  Tom and Mary are looking for John .\n",
            "Y labels  Do you have a pet ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One head of self attention\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size, mask = True):\n",
        "        super().__init__()\n",
        "        # Query, key, and value are all linear layers.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        # create a tril matrix of ones\n",
        "        # PyTorch naming convention because the tril is not a parameter\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mask = mask\n",
        "    def forward(self, x, query = None, key = None, value = None):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        if key is None:\n",
        "            key = x\n",
        "        if query is None:\n",
        "            query = x\n",
        "        if value is None:\n",
        "            value = x\n",
        "\n",
        "        k = self.key(key)   # (B, T, head_size)\n",
        "        q = self.query(query) # (B, T, head_size)\n",
        "        v = self.value(value) # (B, T, head_size)\n",
        "\n",
        "        # print(\"Head: x shape\",x.shape) # (64, 16, 384)\n",
        "        # print(\"Head: query shape\", q.shape) # (64, 16, 64)\n",
        "        # print(\"Head key shape\", k.shape) # (64, 19, 64)\n",
        "        # print(\"Head value shape\", v.shape) # (64, 19, 64)\n",
        "# 64 x 19 x 64\n",
        "# 64 x 64 x 16\n",
        "# w = 64 x 19 x 16\n",
        "# 64 x 16 x 19\n",
        "# v = 64 x 19 x 64\n",
        "# Required = 64 x 16 x 64\n",
        "        # Dot product the key and the query to get the weights\n",
        "        w = k @ q.transpose(-2, -1)  # (B,T,H) @ (B,H,T) = (B, T, T)\n",
        "\n",
        "        # Dividing by sqrt(head_size) for stability and making sure the variance stays close to zero\n",
        "        w = w * (C ** -0.5)\n",
        "\n",
        "        if self.mask:\n",
        "            w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        w = F.softmax(w, dim = -1)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        out = w.transpose(-2, -1) @ v # (B, T, T) @ (B, T, C) = (B, T, C) cuz B stays the same so essentially its a (T, T) @ (T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, no_of_heads, head_size, mask = True):\n",
        "        super().__init__()\n",
        "        self.mask = mask\n",
        "        self.heads = nn.ModuleList([Head(head_size, self.mask) for _ in range(no_of_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, query = None, key = None, value = None):\n",
        "        if key is None:\n",
        "            key = x\n",
        "        if query is None:\n",
        "            query = x\n",
        "        if value is None:\n",
        "            value = x\n",
        "        out = torch.cat([head(x, query, key, value) for head in self.heads], dim = -1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "    def forward(self, x):\n",
        "            x = x + self.net(x)\n",
        "            x = self.ln(x)\n",
        "            return x\n",
        "\n",
        "class GlobalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd, no_of_heads, mask = True):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // no_of_heads\n",
        "        self.mask = mask\n",
        "        self.mha = MultiHeadAttention(no_of_heads, head_size, self.mask)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.mha(x)\n",
        "        x = self.ln(x)\n",
        "        return x\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, n_embd, no_of_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // no_of_heads\n",
        "        self.ca = MultiHeadAttention(no_of_heads, head_size, mask = False)\n",
        "        self.ln = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = x + self.ca(x=x, query=x, key=context, value=context)\n",
        "        x = self.ln(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_embd, no_of_heads):\n",
        "        super().__init__()\n",
        "        self.sa = GlobalSelfAttention(n_embd, no_of_heads, mask = False)\n",
        "        self.ffn = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "# class PositionalEncoding(nn.Module):\n",
        "#     def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "#         super(PositionalEncoding, self).__init__()\n",
        "#         den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "#         pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "#         pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "#         pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "#         pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "#         pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "#     def forward(self, token_embedding):\n",
        "#         return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[EncoderLayer(n_embd=n_embd, no_of_heads=no_of_heads) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_embd, no_of_heads):\n",
        "        super().__init__()\n",
        "        self.masked_attn = GlobalSelfAttention(n_embd, no_of_heads, mask = True)\n",
        "        self.crs_attn = CrossAttention(n_embd = n_embd, no_of_heads = no_of_heads)\n",
        "        self.ffn = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        x = self.masked_attn(x)\n",
        "        # print(\"CA: Shape of x\", x.shape)\n",
        "        # print(\"CA: Shape of context\", context.shape)\n",
        "        x = self.crs_attn(x = x, context = context)\n",
        "        # print(\"Decoder\")\n",
        "        x = self.ffn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.blocks = nn.Sequential(*[DecoderLayer(n_embd=n_embd, no_of_heads=no_of_heads) for _ in range(n_layer)])\n",
        "\n",
        "    def forward(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "        B, T = x.shape\n",
        "        tok_emb = self.token_embedding_table(x)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.dropout(x)\n",
        "        for i in range(n_layer):\n",
        "            x  = self.blocks[i](x, context)\n",
        "        # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size_src, vocab_size_tgt, n_embd):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size_src)\n",
        "        self.decoder = Decoder(vocab_size_tgt)\n",
        "        self.final_layer = nn.Linear(n_embd, vocab_size_tgt)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        context, outputs = inputs\n",
        "        context = context.to(device)\n",
        "        outputs = outputs.to(device)\n",
        "        y_input = outputs[:, :-1]\n",
        "        y_labels = outputs[:, 1:]\n",
        "\n",
        "        context = self.encoder(context)\n",
        "        # print(\"output of context = \", context.shape)\n",
        "        # print(\"Y input \", y_input.shape)\n",
        "        x = self.decoder(x = y_input, context = context)  # (batch_size, target_len, d_model)\n",
        "        logits = self.final_layer(x) # B, T, vocab_size_tgt\n",
        "\n",
        "        B,T,C = logits.shape\n",
        "\n",
        "        logits = logits.view(B * T, C)\n",
        "        # print(\"logits shape = \", logits.shape)\n",
        "        y_labels = y_labels.reshape(B * T)\n",
        "        # print(\"y labels shape =\", y_labels.shape)\n",
        "        loss = F.cross_entropy(logits, y_labels)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, src_sentence, max_new_tokens):\n",
        "        idx = torch.tensor([2], device = device).view(1, -1) # token 2 is <bos> token\n",
        "        encoded_sentence = encode_sentence(src_sentence, SRC_LANGUAGE, token_transform[SRC_LANGUAGE], vocab_transform[SRC_LANGUAGE])\n",
        "        encoded_sentence = torch.tensor(encoded_sentence, device = device).view(1, -1)\n",
        "        # print(encoded_sentence.shape)\n",
        "        context = self.encoder(encoded_sentence)\n",
        "        #idx is (B,T)\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Cropping the idx to the last block_size tokens\n",
        "            idx_cond = idx[:, :block_size]\n",
        "            x = self.decoder(x = idx_cond, context = context)\n",
        "            logits = self.final_layer(x) # B, T, vocab_size_tgt\n",
        "            logits = logits[:, -1, :] # Becomes (B, C)\n",
        "            probs = F.softmax(logits, dim = -1)\n",
        "\n",
        "            # Sampling from distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "            idx = torch.cat((idx, idx_next), dim = 1)\n",
        "        idx = idx.tolist()\n",
        "        return decode_sequence(idx[0], TGT_LANGUAGE, vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class BigramLanguageModel(nn.Module):\n",
        "#     def __init__(self, vocab_size):\n",
        "#         super().__init__()\n",
        "#         self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "#         self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "#         self.blocks = nn.Sequential(*[Block(n_embd, no_of_heads=no_of_heads) for _ in range(n_layer)])\n",
        "#         self.ln_f = nn.LayerNorm(n_embd)\n",
        "#         self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "#     def forward(self, idx, targets = None):\n",
        "#         B,T = idx.shape\n",
        "#         tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "#         pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n",
        "#         x = tok_emb + pos_emb\n",
        "#         x = self.blocks(x)\n",
        "#         x = self.ln_f(x)\n",
        "#         logits = self.lm_head(x)\n",
        "#         if targets is None:\n",
        "#             loss = None\n",
        "#         else:\n",
        "#             # idx and targets are of shape (B,T)\n",
        "#             B,T,C = logits.shape\n",
        "#             logits = logits.view(B * T, C)\n",
        "#             targets = targets.view(B * T)\n",
        "#             loss = F.cross_entropy(logits, targets)\n",
        "#         return logits, loss\n",
        "\n",
        "#     def generate(self, idx, max_new_tokens):\n",
        "#         #idx is (B,T)\n",
        "#         for _ in range(max_new_tokens):\n",
        "#             # Cropping the idx to the last block_size tokens\n",
        "#             idx_cond = idx[:, -block_size:]\n",
        "#             logits, loss = self(idx_cond)\n",
        "#             logits = logits[:, -1, :] # Becomes (B, C)\n",
        "#             probs = F.softmax(logits, dim = -1)\n",
        "\n",
        "#             # Sampling from distribution\n",
        "#             idx_next = torch.multinomial(probs, num_samples = 1)\n",
        "#             idx = torch.cat((idx, idx_next), dim = 1)\n",
        "#         return idx\n",
        "\n",
        "# model = BigramLanguageModel(vocab_size)\n",
        "# m = model.to(device)\n",
        "# logits, loss = m(xb, yb)\n",
        "# print(loss)\n",
        "\n",
        "# idx = torch.zeros((1,1), dtype = torch.long, device = device) # stands for the new line token \\n\n",
        "# print(decode(m.generate(idx = idx, max_new_tokens = 100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "OfINxkfCZfun"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test for encoder\n",
        "x, y = next(iter(train_dataloader))\n",
        "sample_encoder = Encoder(vocab_size=vocab_size_src)\n",
        "# x,y = next(iter(train_dataloader))\n",
        "with torch.no_grad():\n",
        "    sample_encoder_output = sample_encoder(x)\n",
        "\n",
        "print(\"X shape = \", x.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(B,T,C)`.\n",
        "\n",
        "# Unit test for decoder\n",
        "print(sample_encoder_output.shape)\n",
        "sample_decoder = Decoder(vocab_size=vocab_size_tgt)\n",
        "\n",
        "print(\"y.shape = \", y.shape)\n",
        "with torch.no_grad():\n",
        "    output = sample_decoder(\n",
        "        x=y,\n",
        "        context=torch.randn((y.shape[0], y.shape[1], n_embd)))\n",
        "\n",
        "# Print the shapes.\n",
        "print(y.shape)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "8-tWYsvWuk2H",
        "outputId": "af418ae3-9c96-47d3-8aaa-6d45de8ff545"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'int' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b911910a3c3d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Unit test for encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msample_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# x,y = next(iter(train_dataloader))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit test for Transformer\n",
        "transformer = Transformer(vocab_size_src = vocab_size_src, vocab_size_tgt = vocab_size_tgt, n_embd = n_embd)\n",
        "transformer = transformer.to(device)\n",
        "logits, loss = transformer((x, y))\n",
        "\n",
        "print(\"Hi\")\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5-jtuT07h9H",
        "outputId": "b5cc58a6-5575-49ff-a717-251b2f7e81b5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi\n",
            "torch.Size([64, 18])\n",
            "torch.Size([64, 21])\n",
            "torch.Size([1280, 14875])\n",
            "tensor(9.8131, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample generation\n",
        "# idx = torch.zeros((1,1), dtype = torch.long, device = device) # stands for the new line token \\n\n",
        "sentence = transformer.generate(src_sentence = \"Comment allez vous?\", max_new_tokens = 3)\n",
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3YzC9oymYyI",
        "outputId": "56f56293-5488-42e2-d547-8adbe29f9637"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " nice nutmeg rational\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    transformer.eval()\n",
        "    total_samples = len(val_dataloader)\n",
        "    losses = torch.zeros(total_samples)\n",
        "    for i, data in enumerate(val_dataloader):\n",
        "        logits, loss = transformer(data)\n",
        "        # print(loss)\n",
        "        losses[i] = loss.item()\n",
        "        # print(i)\n",
        "    out = losses.mean()\n",
        "    transformer.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "1x1WxVDem-E9"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(transformer.parameters(), lr = 3e-4)"
      ],
      "metadata": {
        "id": "f-Tmsy7ntQS9"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = torch.zeros(len(train_dataloader), requires_grad=False)\n",
        "for iter in range(max_iters):\n",
        "    if iter != 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"Step {iter}: Train loss = {train_losses.mean()} Val loss = {losses}\")\n",
        "        train_losses = torch.zeros(len(train_dataloader), requires_grad=False)\n",
        "    # Iterate over the batches in the train_dataloader\n",
        "    for i, (X, Y) in tqdm(enumerate(train_dataloader)):\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "        logits, loss = transformer((X, Y))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()\n",
        "        train_losses[i] = loss\n",
        "\n",
        "# for iter in range(max_iters):\n",
        "#     if iter % eval_interval == 0:\n",
        "#       losses = estimate_loss()\n",
        "#       print(f\"Step {iter}: Train loss = {losses['train']}, Val loss = {losses['val']}\")\n",
        "\n",
        "#     xb, yb = get_batch(\"train\")\n",
        "#     logits, loss = m(xb, yb)\n",
        "#     optimizer.zero_grad(set_to_none = True)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()"
      ],
      "metadata": {
        "id": "RPGRHeKItS9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a8e4ea2-9a08-4070-d3cf-8fab1ede5a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "327it [00:58,  6.32it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z2B3vXPMh8dv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}